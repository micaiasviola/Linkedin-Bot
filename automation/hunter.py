import os
import json
import re
import time
import urllib.parse
import asyncio
import datetime
import random  # Import essencial para a "humaniza√ß√£o" do bot (Jitter)
from playwright.async_api import async_playwright

# ================= CONFIGURA√á√ïES GERAIS =================
# Caminho absoluto pra garantir que funcione no Windows sem dor de cabe√ßa
USER_DATA_DIR = os.path.abspath(os.path.join(os.getcwd(), "navegador_robo"))
CAMINHO_HISTORICO = os.path.join(os.getcwd(), "data", "historico_vagas.json")

RESULTADOS_POR_PAGINA = 25
MAX_PAGINAS_SEM_NOVIDADE = 2  # Se passar 2 p√°ginas s√≥ com vaga velha, a gente para pra n√£o perder tempo

# --- SISTEMA DE RANKING (A IA DO PENTE FINO) ---
# Se tiver isso no t√≠tulo, ganha ponto
KEYWORDS_POSITIVAS = [
    "python", "django", "flask", "fastapi", "pandas", 
    "junior", "j√∫nior", "jr", "estagio", "est√°gio", "trainee", "entry level"
]

# Se tiver isso, perde ponto (mas n√£o √© eliminado na hora)
KEYWORDS_NEGATIVAS = [
    "senior", "pleno" 
]

# --- O FILTRO "ANTI-S√äNIOR" ---
# Regex parruda pra barrar vaga que pede Tech Lead pagando de Jr.
BLACKLIST_RE = re.compile(
    r"(senior|s√™nior|sr\.?|pleno|lead|tech lead|l√≠der|principal|staff|head|manager|gerente|gestor|coordenador|expert|architect|arquiteto|\biii\b|\biv\b|\bv\b)",
    re.I
)

# Pega o ID num√©rico da vaga na URL
JOB_ID_RE = re.compile(r"/jobs/view/(\d+)")

# ================= LOGGING (FICA BONITO NO TERMINAL) =================
def log_terminal(msg, tipo="INFO"):
    """
    Fun√ß√£ozinha pra colorir o terminal. Ajuda muito no debug visual
    enquanto o Streamlit t√° rodando no navegador.
    """
    now = datetime.datetime.now().strftime("%H:%M:%S")
    # C√≥digos ANSI para cores
    RESET = "\033[0m"
    BOLD = "\033[1m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    RED = "\033[91m"
    CYAN = "\033[96m"
    WHITE = "\033[97m"
    
    prefix = f"{BOLD}[{now}]{RESET}"
    if tipo == "INFO": print(f"{prefix} {CYAN}‚ÑπÔ∏è  {msg}{RESET}")
    elif tipo == "SUCCESS": print(f"{prefix} {GREEN}‚úÖ {msg}{RESET}")
    elif tipo == "WARN": print(f"{prefix} {YELLOW}‚ö†Ô∏è  {msg}{RESET}")
    elif tipo == "ERROR": print(f"{prefix} {RED}‚ùå {msg}{RESET}")
    elif tipo == "DEBUG": print(f"{prefix} {WHITE}üîß {msg}{RESET}")

# ================= GERENCIAMENTO DE ESTADO (JSON) =================
def carregar_historico_global():
    # Se n√£o tiver arquivo, come√ßa do zero
    if not os.path.exists(CAMINHO_HISTORICO): return set()
    try:
        with open(CAMINHO_HISTORICO, "r", encoding="utf-8") as f:
            dados = json.load(f)
            # Garante que retorna um set pra busca ser O(1)
            if isinstance(dados, list): return {l.rstrip("/") for l in dados}
            return set()
    except: return set()

def salvar_historico_global(historico: set):
    # Cria a pasta data se o usu√°rio deletou sem querer
    os.makedirs(os.path.dirname(CAMINHO_HISTORICO), exist_ok=True)
    with open(CAMINHO_HISTORICO, "w", encoding="utf-8") as f:
        # Salva ordenado pra ficar f√°cil de ler se abrir no bloco de notas
        json.dump(sorted(list(historico)), f, indent=2)

# ================= L√ìGICA DE SCORE =================
def calcular_score_detalhado(titulo):
    """
    Aqui a gente define se a vaga √© 'Quente' ou 'Fria'.
    Retorna a nota (0-100) e o motivo pra exibir na UI.
    """
    score = 50 # Come√ßa neutro
    detalhes = []
    titulo_lower = titulo.lower()
    
    # Bonifica√ß√£o
    for word in KEYWORDS_POSITIVAS:
        if word in titulo_lower: 
            score += 15
            detalhes.append(f"{word}")
            
    # Penaliza√ß√£o
    for word in KEYWORDS_NEGATIVAS:
        if word in titulo_lower: 
            score -= 20
            detalhes.append(f"-{word}")
            
    # Trava entre 0 e 100 pra n√£o quebrar o CSS depois
    final_score = max(0, min(100, score))
    motivo_str = ", ".join(detalhes) if detalhes else "Base(50)"
    return final_score, motivo_str

def normalizar_link(href: str):
    # Limpa aquelas URLs sujas do LinkedIn cheias de tracking params
    if not href: return None
    match = JOB_ID_RE.search(href)
    if not match: return None
    return f"https://www.linkedin.com/jobs/view/{match.group(1)}"

# ================= CORE DO ROB√î (ASYNC) =================
async def _buscar_vagas_async(historico, termo_usuario, filtro_tempo, max_paginas, salvar_historico, queue, ordenar_por_data=False):
    context = None # Inicializa vazio pra evitar erro no finally
    try:
        query = termo_usuario or "Desenvolvedor Junior"
        # O pulo do gato: j√° filtra Senior na query pro LinkedIn nem trazer lixo
        q = urllib.parse.quote(f"{query} NOT (Senior OR Pleno OR Lead)")
        
        log_terminal(f"=== INICIANDO HUNTER PRO (MODO HUMANIZADO) ===", "INFO")
        
        paginas_sem_novidade = 0
        novos_links_sessao = set()

        async with async_playwright() as p:
            log_terminal("Abrindo navegador...", "INFO")
            # Usa contexto persistente pra manter cookies e sess√£o logada (menos chance de captcha)
            context = await p.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR, headless=False, channel="chrome",
                args=["--start-maximized", "--disable-blink-features=AutomationControlled"], viewport=None
            )
            
            # Reutiliza a aba aberta se tiver, sen√£o cria nova
            if len(context.pages) > 0: page = context.pages[0]
            else: page = await context.new_page()

            for pagina in range(max_paginas):
                # Se o usu√°rio clicou em Parar no App, isso aqui levanta exce√ß√£o e mata o loop
                if asyncio.current_task().cancelled(): raise asyncio.CancelledError()

                # --- ESTRAT√âGIA ANTI-BAN (HUMANIZA√á√ÉO) ---
                # Rob√¥ n√£o clica na p√°g 2 em 0.1ms. A gente espera um pouco.
                if pagina > 0:
                    pausa = random.uniform(2.1, 4.5)
                    log_terminal(f"Lendo p√°gina {pagina}... (Pausa humana de {pausa:.1f}s)", "DEBUG")
                    await asyncio.sleep(pausa)

                offset = pagina * RESULTADOS_POR_PAGINA
                base_url = f"https://www.linkedin.com/jobs/search?keywords={q}&location=Brazil&geoId=106057199&f_AL=true&f_TPR={filtro_tempo}&start={offset}"
                
                # Se o usu√°rio quer novidade, for√ßa ordena√ß√£o por DATA (fura o algoritmo de relev√¢ncia)
                if ordenar_por_data: base_url += "&sortBy=DD"
                
                log_terminal(f"--- Processando P√ÅGINA {pagina + 1} ---", "INFO")
                # Manda aviso pra UI
                await queue.put(([], f"üîÑ Lendo p√°gina {pagina + 1}..."))
                
                try:
                    await page.goto(base_url, timeout=30000)
                    
                    # --- SCROLL IMPERFEITO (HUMANIZA√á√ÉO PT. 2) ---
                    # Nada de rolar fixo. Varia a quantidade e o tempo pra parecer uma pessoa lendo.
                    steps = random.randint(3, 5)
                    for _ in range(steps): 
                        scroll_amount = random.randint(700, 1200)
                        await page.mouse.wheel(0, scroll_amount)
                        await asyncio.sleep(random.uniform(0.5, 1.2))
                        
                except Exception as e:
                    log_terminal(f"Erro navega√ß√£o: {e}", "ERROR")
                    await queue.put(([], f"‚ö†Ô∏è Erro navega√ß√£o: {e}"))
                    continue

                # Seleciona todas as vagas vis√≠veis
                links_el = await page.locator("a[href*='/jobs/view/']").all()
                total_encontrados = len(links_el)
                log_terminal(f"Links na p√°gina: {total_encontrados}", "DEBUG")

                # Se achou pouco link, provavelmente o LinkedIn bloqueou ou acabou a lista
                if total_encontrados < 2:
                    log_terminal("Fim da lista detectado.", "WARN")
                    await queue.put(([], f"‚ö†Ô∏è Fim da lista detectado."))
                    break

                novas = []
                count_sucesso = 0
                
                # Processa cada vaga encontrada
                for el in links_el:
                    href = await el.get_attribute("href")
                    titulo = (await el.inner_text() or "Vaga sem titulo").strip()
                    link = normalizar_link(href)
                    
                    if not link: continue
                    
                    # J√° vimos essa? Pula.
                    if link in historico: continue
                    
                    # Passou no filtro anti-s√™nior?
                    if BLACKLIST_RE.search(f"{titulo} {link.lower()}"): continue

                    score, motivo = calcular_score_detalhado(titulo)

                    historico.add(link)
                    novos_links_sessao.add(link)
                    novas.append({"titulo": titulo, "link": link, "score": score, "motivo": motivo})
                    count_sucesso += 1

                log_terminal(f"Resumo P√°g {pagina+1}: {count_sucesso} Novas", "SUCCESS" if count_sucesso else "INFO")

                if novas:
                    # Ordena as da p√°gina atual antes de mandar
                    novas.sort(key=lambda x: x['score'], reverse=True)
                    await queue.put((novas, f"‚úÖ P√°g {pagina+1}: +{len(novas)} vagas"))
                    paginas_sem_novidade = 0
                else:
                    await queue.put(([], f"‚ö™ P√°g {pagina+1}: Sem novidades"))
                    paginas_sem_novidade += 1

                # Prote√ß√£o pra n√£o ficar rodando infinito se n√£o tiver nada novo
                if paginas_sem_novidade >= MAX_PAGINAS_SEM_NOVIDADE:
                    log_terminal("Parando busca (Sem novidades).", "WARN")
                    await queue.put(([], f"‚úã Parando (Sem novidades)."))
                    break
            
        # Salva tudo no final se o usu√°rio deixou
        if salvar_historico and novos_links_sessao:
            salvar_historico_global(historico)
            log_terminal(f"Banco de dados atualizado (+{len(novos_links_sessao)} vagas).", "SUCCESS")
            await queue.put(([], "üíæ Salvo no disco."))

    except asyncio.CancelledError:
        log_terminal("Tarefa cancelada pelo usu√°rio!", "WARN")
    except Exception as e:
        log_terminal(f"Erro Cr√≠tico: {str(e)}", "ERROR")
        await queue.put(([], f"‚ùå Erro: {str(e)}"))
    finally:
        # BLINDAGEM: Garante que o navegador fecha de qualquer jeito
        if context:
            try: await context.close()
            except: pass
        # Manda sinal de fim pra UI n√£o ficar travada
        await queue.put(None)

# ================= PONTE SYNC -> ASYNC =================
def buscar_vagas_em_lote(links_ja_vistos, termo, tempo, salvar, max_pg=10, ordenar_por_data=False):
    """
    Essa fun√ß√£o √© o wrapper pro Streamlit (que √© s√≠ncrono) conseguir
    conversar com o Playwright (que √© ass√≠ncrono).
    """
    historico = carregar_historico_global()
    historico.update(links_ja_vistos)
    
    queue = asyncio.Queue()
    
    # Gambiarra padr√£o pra pegar o loop no Windows/Streamlit
    try: loop = asyncio.get_event_loop()
    except RuntimeError: loop = asyncio.new_event_loop(); asyncio.set_event_loop(loop)

    # Dispara o rob√¥ em background
    task = loop.create_task(_buscar_vagas_async(historico, termo, tempo, max_pg, salvar, queue, ordenar_por_data))

    try:
        while True:
            # Fica ouvindo a fila. O run_until_complete faz a ponte sync.
            dados = loop.run_until_complete(queue.get())
            if dados is None: break
            yield dados
    except GeneratorExit:
        # Se o usu√°rio clicar em PARAR no app.py, o loop quebra e cai aqui.
        # A gente cancela a tarefa pra fechar o navegador imediatamente.
        log_terminal("Interrompendo tarefa ass√≠ncrona...", "WARN")
        task.cancel()
        loop.run_until_complete(task)
        raise